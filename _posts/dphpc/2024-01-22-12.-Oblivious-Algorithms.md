---
tags: [Algorithms, Oblivious-algorithms, Parallel-computing, Multithread]
title: "Oblivious Algorithms"
categories: dphpc lecture-notes
math: true
---

# Primer for Parallel Algorithms

Back to basics: The execution of programs can be described with Execution DAGs (EDAG)

- Each vertex is a basic calculation, and each edge is a dependency (value flowing)
- Recall:
  - Work W – number of operations performed when executing the algorithm (= sequential running time for $P=1$)
  - Depth D – minimal number of operations for any parallel execution (= parallel running time for $P=∞$)
    - Depth is also the longest path in the EDAG

![](/assets/img/ScreenShot%202024-01-11%20at%2018.59.43.png)

## Parallel Summation (“Reduction”) – example for N=9

![](/assets/img/ScreenShot%202024-01-11%20at%2019.07.06.png)

![](/assets/img/ScreenShot%202024-01-11%20at%2019.07.25.png)

![](/assets/img/ScreenShot%202024-01-12%20at%2016.49.20.png)

# Work/Depth in Practice – Oblivious Algorithms

> An algorithm is data-oblivious if, for each problem size, the sequence of instructions executed, the set of memory locations read, and the set of memory locations written by each executed instruction are determined by the input size and are independent of the values of the other inputs
{: .prompt-tip}

![](/assets/img/ScreenShot%202024-01-12%20at%2016.50.22.png)

|                                   |     |
| --------------------------------- | --- |
| Quicksort?                        | No  |
| Prefix sum on an array?           | Yes |
| Dense matrix multiplication?      | Yes |
| Sparse matrix vector product?     | No  |
| Dense matrix vector product?      | Yes |
| Queue-based breadth-first search? | No  |

## Can an algorithm decide whether a program is oblivious or not?

Answer: no, proof similar to decision problem whether a program always outputs zero or not

# Structural obliviousness as stronger property

> A program is structurally-oblivious if any value used in a conditional branch, and any value used to compute indices or pointers is structurally-dependent only in the input variable(s) that contain the problem size, but not on any other input
{: .prompt-tip}

![](/assets/img/ScreenShot%202024-01-12%20at%2016.52.44.png)

Clear that structurally oblivious programs are also data oblivious

- Can be programmatically (statically decided)
- Sufficient for practical use

The middle example is **not** structurally oblivious but data oblivious

- First branch is always taken (assuming no overflow) but static dependency analysis is conservative

# Why obliviousness?

We can easily reason about oblivious algorithms

- Execution DAG can be constructed “statically” – we can even build circuits for fixed N (HW acceleration!)
- We have done this intuitively, but we never looked at BFS for example

Recap: simple example (that you know): parallel summation

- Question: what is W(n) and D(n) of sequential summation?
  - W(n)=D(n)=n-1
- Question: is this optimal? How would you define optimality?
  - Separate for W and D! Typically try to achieve both optimally!
- Question: what is W(n) and D(n) of the optimal parallel summation?
  - $W(n)=n-1$ $D(n)=⌈\log_2 𝑛⌉$
  - Are both W and D optimal?
    - Yes!

## Scan

![](/assets/img/ScreenShot%202024-01-12%20at%2016.54.25.png)

### Algorithm

![](/assets/img/ScreenShot%202024-01-12%20at%2016.56.39.png)

- work? (hint: after the way up, all powers of two are done, all others require another operation each)
  - $𝑊 (𝑛) = 2𝑛 − \log_2 𝑛 − 1$
- depth? (needs to go up and down the tree)
  - $D (𝑛) = 2 \log_2 𝑛 − 1$

## Dissemination/recursive doubling

Recursive to get to $𝑾 = 𝑶 (𝒏)$ and $𝑫 = 𝑶(\log (𝒏))$! (Assume $𝒏 = 𝟐𝒌$ for simplicity!)

- Sounds “optimal”, doesn’t it? Well, let’s look at the constants!

![](/assets/img/ScreenShot%202024-01-12%20at%2016.55.23.png)

- work? (hint: count number of omitted ops)
  - $𝑊 (𝑛) = 𝑛 \log_2 𝑛 − 𝑛 + 1$
- depth?
  - $D (𝑛) = \log_2 𝑛$

## is there a depth- and work-optimal algorithm?

The answer is surprisingly: no

- We know, for parallel prefix: $𝑊 + 𝐷 ≥ 2𝑛 − 2$

![](/assets/img/ScreenShot%202024-01-12%20at%2017.01.31.png)

# Work-Depth Tradeoffs and deficiency

> [!NOTE] Deficiency “The deficiency of a prefix circuit 𝑐 is defined as $def (𝑐) = 𝑊_𝑐 + 𝐷_𝑐− (2𝑛 − 2)$ ![](/assets/img/ScreenShot%202024-01-12%20at%2017.02.20.png)

# Work- and depth-optimal constructions

Work-optimal?

- Only sequential! Why?
- $𝑊 = 𝑛 − 1$, thus $𝐷 = 2𝑛 − 2 − 𝑊 = 𝑛 − 1$ q.e.d.

Depth-optimal?

- Ladner and Fischer propose a construction for work-efficient circuits with minimal depth
  - $𝐷 = ⌈log2 𝑛⌉, 𝑊 ≤ 4𝑛$
  - Simple set of recursive construction rules
  - Has an unbounded fan-out! May thus not be practical

Depth-optimal with bounded fan-out?

- Some constructions exist, interesting open problem

# Why do we care about this prefix sum so much?

It’s the simplest problem to demonstrate and prove W-D tradeoffs

- And it’s one of the most important parallel primitives

Prefix summation as function composition is extremely powerful!

- Many seemingly sequential problems can be parallelized!

Simple first example: binary adder – 𝑠 = 𝑎 + 𝑏 (n-bit numbers)

- Starting with single-bit (full) adder for bit i ![](/assets/img/ScreenShot%202024-01-12%20at%2017.06.13.png)

## Seems very sequential, can this be parallelized?

We only want 𝒔!

- Requires $𝑐_{𝑖𝑛,1}, 𝑐_{𝑖𝑛,2}, … , 𝑐_{𝑖𝑛,𝑛}$ though ![](/assets/img/ScreenShot%202024-01-12%20at%2017.07.27.png)

# Prefix sums as magic bullet for other seemingly sequential algorithms

Any time a sequential chain can be modeled as function composition! ![](/assets/img/ScreenShot%202024-01-12%20at%2017.13.17.png) ![](/assets/img/ScreenShot%202024-01-12%20at%2017.13.32.png)

## Another use for prefix sums: Parallel radix sort

![](/assets/img/ScreenShot%202024-01-12%20at%2017.15.56.png)

# Oblivious graph algorithms

Seems paradoxical but isn’t (may just not be most efficient)

- Use adjacency matrix representation of graph – “compute with zeros”

![](/assets/img/ScreenShot%202024-01-12%20at%2017.16.28.png)

# Algebraic semirings

A semiring is an algebraic structure that

- Has two binary operations called “addition” and “multiplication” on a set S
- Addition must be associative $((a+b)+c = a+(b+c))$ and commutative $a+b=b+a$ and have an identity element
- Multiplication must be associative and have an identity element
- Multiplication distributes over addition $(a*(b+c) = a*b+a*c)$
  - →Multiplication by additive identity annihilates
- Semirings are denoted by tuples (S, “+”, “*”, additive identity, multiplicative identity)
  - “Standard” ring of rational numbers: $(ℝ, +, *, 0, 1)$
  - Boolean semiring: $({0,1}, ∨, ∧, 0, 1)$
  - Tropical semiring: $(ℝ ∪ {∞}, min, +, ∞, 0)$ (also called min-plus semiring)

## Oblivious shortest path search on $(ℝ ∪ {∞}, min, +, ∞, 0)$

Construct distance matrix from adjacency matrix by replacing all off-diagonal zeros with ∞

- Initialize distance vector 𝒅𝟎of size n to ∞ everywhere but zero at start vertex
- E.g., $𝐝𝟎 = (∞, 𝟎, ∞, ∞, ∞, ∞ )^𝑻$ Show evolution when multiplied!
- SSSP can be performed with $n+1$ matrix-vector multiplications!
- Question: total work and depth?
  - $𝑊 = 𝑂(𝑛3), 𝐷 = 𝑂(𝑛 \log 𝑛)$
- Question: Is this good? Optimal?
  - Nope:
    - Dijkstra = $𝑂( 𝐸 + 𝑉 log 𝑉 )$

## Oblivious connected components on $({0,1}, ∨, ∧, 0, 1)$

Question: How could we compute the transitive closure of a graph?

- Multiply the matrix $(𝐴 + 𝐼) 𝑛$ times with itself in the Boolean semiring!
- Why? Demonstrate that $(𝐴 + 𝐼)^2$ has 1s for each path of at most length $1$ By induction show that $(𝐴 + 𝐼)^ 𝑘$ has 1s for each path of at most length $k$
- What is work and depth of transitive closure?
- Repeated squaring!
- $𝑊 = 𝑂(𝑛^3\log 𝑛)$ $𝐷 = 𝑂(\log_2𝑛)$
  - $\lceil \log_2 𝑛\rceil$ multiplications (think $𝐴^4 = {𝐴^2}^2$ )

How to get to connected components from a transitive closure matrix?

- Each component needs unique label
- Create label matrix $𝐿_{𝑖𝑗} = 𝑗$ iff $(𝐴𝐼)^𝑛_{𝑖𝑗}$ = 1 and $𝐿_{𝑖𝑗} = ∞$ otherwise
- For each column (vertex) perform min-reduction to determine its component label!
- Overall work and depth?
  - $𝑊 = 𝑂(𝑛^3\log 𝑛)$, $𝐷 = 𝑂(\log_2𝑛)$

![](/assets/img/ScreenShot%202024-01-12%20at%2017.22.33.png)
