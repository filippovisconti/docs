---
tags: [Cuda, Parallel-computing, Multithread]
title: "CUDA"
categories: dphpc lecture-notes
math: true
---

# GPU Hardware Model

GPUs devote more hardware to computations, not as general-purpose as a CPU

- Optimised for parallelism (low depth workloads)

Used as an accelerator to a host system

- "Throughput compute unit"

Device is built from **Streaming Multiprocessors** (SMs) Scalable caching mechanism (not all coherent!) Communication with host system via PCIe bus or NVLINK.

![](/assets/img/ScreenShot%202024-01-04%20at%2016.46.03.png)

## The Streaming Multiprocessors

ALUs divided into specific functions

- Can run concurrently
- Hardware can fast-switch between active "threads" to hide latency

Multiple levels of instruction and data caching

- `L0` cache for instructions

Scratch-pad ("shared") memory controlled by the programmer. Specialised units for specific operations

- Tensor Cores: ![](/assets/img/ScreenShot%202024-01-04%20at%2016.47.26.png) ![](/assets/img/ScreenShot%202024-01-04%20at%2016.47.40.png)

## GPU Programming Model – Memory

Memory explicitly allocated on the GPU

- "Managed" memory exists, but slower

Data copied from host → GPU explicitly Optimization: pinned memory

- Accelerates later memory copies, enables asynchronous copies

> Question: Are memory writes to pinned memory visible to the GPU's async memcpy?
>
> - Answer: Not necessarily without fences and the `volatile` keyword
> - pinning is an OS concept and does not impact the memory model

## GPU Programming Model – Execution

GPU-compiled functions written separately from host code

- Both can coexist in `.cu` files, no GPU code in `.c/.cpp` files

NVIDIA CUDA is (mostly) compatible with `C++14`. >Code is asynchronously executed.

- Streams act as "command queues" to the GPU
- Events can be queued onto the stream for synchronisation

Threads execute "together" and are arranged in blocks

- Invocation: `func<<numBlocks, threadsPerBlock>>` ![](/assets/img/ScreenShot%202024-01-04%20at%2016.50.03.png)

### Threads and blocks

A group of threads is called "block"

- Maximum $1024$ threads per block
- A block executes on one SM – threads share registers
- One SM can run multiple blocks (if they fit)
- All threads in the same block have access to a shared memory – but not across blocks!
- For your convenience, thread and block ids can be 1-, 2-, or 3-dimensional

Threads and blocks live on a logical 3D grid:

- `threadIdx.{x,y,z}` – coordinate of calling thread
- `blockDim.{x,y,z}` – dimension of thread block
- `blockIdx.{x,y,z}` – grid of blocks

Launch parameters `(func<<numBlocks, threadsPerBlock>>)` can be 3D specifications!

- Type "dim3" ![](/assets/img/ScreenShot%202024-01-04%20at%2016.51.21.png)

### Single Instruction, Multiple Threads (SIMT)

A `warp` keeps a single PC and stack

- Composed of $32$ threads

Thread-blocks are composed of multiple warps

- Always execute on the same SM

Warps execute the same instruction, but on different data

- Same logical register maps to different physical register
- Uses prediction for instruction divergence

Warp divergence can cause $32$x slowdown (and more).

## Memory Coalescing

Memory accesses of aligned elements (e.g., `LDG.<X>`) can be shared across threads in a warp

![](/assets/img/ScreenShot%202024-01-04%20at%2017.54.13.png)

## Accelerator programming with CUDA![](/assets/img/ScreenShot%202024-01-04%20at%2015.57.57.png)

# Workflow

1. Copy input data from CPU memory to GPU memory
2. Load GPU program and execute, caching data on chip for performance
3. Copy results from GPU memory to CPU memory

![](/assets/img/Pasted%20image%2020240117175656.png)

## Allocating Memory

Host and Device memory are seperate!!

- You are familiar with host memory management in C: `malloc()` and `free()`.
- On the GPU: `cudaMalloc()` / `cudaFree()` / `cudaMemcpy()` (different args/ret-vals to malloc/free!)
- To make sure we don’t get confused, let’s add suffixes to pointers (not required!)

![](/assets/img/Pasted%20image%2020240117175800.png) ![](/assets/img/Pasted%20image%2020240117175806.png) ![](/assets/img/Pasted%20image%2020240117175816.png)

# Example

## `Vec_add` using N blocks

- Can launch kernel using `<<<N,1>>>` so we use `N` blocks with `1` thread each
- Can access block ID using `blockIdx.x` (use `y`, `z` if grid was declared as 2- or 3-dimensional)

![](/assets/img/Pasted%20image%2020240117180213.png) ![](/assets/img/Pasted%20image%2020240117180219.png)

## `Vec_add` using N threads

- Can launch kernel using`<<<1,N>>>` so we use `N` threads within `1` block
- Can access thread ID using `threadIdx.x` (use `y`, `z` if block was declared as 2- or 3-dimensional)
- Can access block size using `blockDim.x`

# Combining threads and blocks

All threads in a block “compete” for registers!

- Using the maximum ($1024$) number of threads per block might not be a good idea if you use a lot of local variables!
- Threads within a block can communicate using (fast) shared memory.
- How can we combine thread and block indexes?

![](/assets/img/Pasted%20image%2020240117180349.png)

> Use built-in variable `blockDim.x` to get number of threads per block. ![](/assets/img/Pasted%20image%2020240117180416.png)

# Shared Memory between threads

Can be declared (in kernel) using **`__shared__`**

- Only allocated once per block!
- Not visible to threads in other blocks!

## 1D Stencil Kernel

![](/assets/img/Pasted%20image%2020240117180447.png)

# Race Condition on Shared Memory -- `__syncthreads()`

After the data is in the `temp` array, how do we make sure threads are synchronized?

- Otherwise thread `RADIUS+1` might read the halo before it is written
- Can use void `__syncthreads();` as a barrier for all threads in a block.
- Be careful when using in conditional code!

Sometimes a global barrier is overkill

- Assume we have a large picture and want to create a histogram of its color values
- Each thread does something like
  - `hist[color[index]] += 1`
- In this case we can use `cudaAtomicAdd(&hist[color[index]], 1)`
  - Also has other atomics, i.e., `CAS`
  - Also works on global memory

# Let’s write a matrix multiplication program

![](/assets/img/Pasted%20image%2020240117191053.png) ![](/assets/img/Pasted%20image%2020240117191059.png)

> [!tip] To improve performance, use local registers (variables) inside kernel functions Declare a `float tmp = 0` variable to accumulate the result and only write back to the `c` matrix at the end!

# Asynchronous Memcpy

![](/assets/img/Pasted%20image%2020240117191823.png)

# Synchroneous vs Asynchroneous Memcpy

- `cudaMemcpy()` blocks until the transfer is complete
- `cudaMemcpy()` only starts when all previous API calls in the same stream have completed
- Kernel launches return immediately
- We need a way to do asynchroneous copies!
  - `cudaMemcpyAsync()` – returns immediately
- Use multiple concurrent streams instead of the implicit default stream

## What is needed to use `cudaStreams`?

- Host memory must be pinned, not pageable – instead of allocating with `malloc`, allocate with `cudaMallocHost(**ptr, size)`
- Create stream with `cudaStream_t stream; cudaStreamCreate(&stream);`
- Add stream as last param to `cudaMemcpyAsync()` and kernel launch
- `cudaStreamSynchronize(stream)` waits for all calls in stream
- `cudaDeviceSynchronize()` syncs all streams

### Streams example - Setup

![](/assets/img/Pasted%20image%2020240117194459.png)

### Streams example – Async operations

![](/assets/img/Pasted%20image%2020240117194511.png)

### Streams example - Validation

![](/assets/img/Pasted%20image%2020240117194525.png)

# Unified Memory

CUDA also supports unified memory

- You allocate it using `cudaMallocManaged()`
- You can access it from the Host and the device!

It lowers the development effort. However, it's harder to predict performance.

## Unified Memory Example

![](/assets/img/Pasted%20image%2020240117194558.png) ![](/assets/img/Pasted%20image%2020240117194604.png)

# CUDA-Aware MPI

Most modern MPI implementations are CUDA aware

- Allow you to pass pointers to device memory to CUDA functions!
- Be mindful of needed synchronization

## CUDA + MPI

![](/assets/img/Pasted%20image%2020240117194704.png)

## CUDA-aware MPI

![](/assets/img/Pasted%20image%2020240117194742.png)
